{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import commentjson as json\n",
    "import imageio.v2 as iio2\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import tinycudann as tcnn\n",
    "import argparse\n",
    "\n",
    "from implicitpleth.models.siren import Siren\n",
    "from implicitpleth.models.combinations import MotionNet\n",
    "from implicitpleth.data.datasets import VideoGridDataset\n",
    "from implicitpleth.utils.utils import trace_video, Dict2Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fix random seed\n",
    "# sd = 0\n",
    "# np.random.seed(sd)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.manual_seed(sd)\n",
    "# random.seed(sd)\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed_all(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./configs/spatiotemporal_residual.json\") as f:\n",
    "    json_config = json.load(f)\n",
    "\n",
    "json_config[\"video_path\"] = \"./assets/v_101_2.avi\"\n",
    "# json_config[\"video_path\"] = \"./assets/v_100_2.avi\"\n",
    "# json_config[\"video_path\"] = \"./assets/v_84_2.avi\"\n",
    "json_config[\"verbose\"] = True\n",
    "json_config[\"append_save_path\"] = None\n",
    "json_config[\"append_load_path\"] = None\n",
    "args = Dict2Class(json_config)\n",
    "args.spatiotemporal_device = torch.device(args.spatiotemporal_device)\n",
    "args.deltaspatial_device = torch.device(args.deltaspatial_device)\n",
    "args.pleth_spatial_device = torch.device(args.pleth_spatial_device)\n",
    "args.pleth_temporal_device = torch.device(args.pleth_temporal_device)\n",
    "args.io_device = torch.device(args.io_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlethSpatioTemporalModel(torch.nn.Module):\n",
    "    def __init__(self, pleth_spatial_encoding, pleth_spatial_network, \n",
    "                 pleth_temporal_encoding, pleth_temporal_network):\n",
    "        super().__init__()\n",
    "\n",
    "        self.spatial_model = tcnn.NetworkWithInputEncoding(pleth_spatial_encoding[\"input_dims\"],\n",
    "                                                           pleth_spatial_network[\"output_dims\"],\n",
    "                                                           pleth_spatial_encoding,\n",
    "                                                           pleth_spatial_network)\n",
    "        self.temporal_model = tcnn.NetworkWithInputEncoding(pleth_temporal_encoding[\"input_dims\"],\n",
    "                                                            pleth_temporal_network[\"output_dims\"],\n",
    "                                                            pleth_temporal_encoding,\n",
    "                                                            pleth_temporal_network)\n",
    "        self.spatial_device = torch.device(\"cpu\")\n",
    "        self.temporal_device = torch.device(\"cpu\")\n",
    "\n",
    "    def forward(self, inp, flag = False):\n",
    "        # All custom models in the repo return 2 values\n",
    "        if flag:\n",
    "            inp = inp.to(self.spatial_device)\n",
    "            spatial_out = self.spatial_model(inp)\n",
    "            return spatial_out, None\n",
    "        else:\n",
    "            inp = inp.to(self.temporal_device)\n",
    "            temporal_out = self.temporal_model(inp)\n",
    "            return temporal_out, None\n",
    "\n",
    "    def set_device(self, spatial_device, temporal_device):\n",
    "        self.spatial_device = spatial_device\n",
    "        self.temporal_device = temporal_device\n",
    "        self.spatial_model.to(self.spatial_device)\n",
    "        self.temporal_model.to(self.temporal_device)\n",
    "\n",
    "    def load_spatial_checkpoint(self, load_path, key='model_state_dict'):\n",
    "        self.spatial_model.load_state_dict(torch.load(load_path)[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(args.motion_model)\n",
    "with open(args.motion_model[\"config\"]) as mmf:\n",
    "    config = json.load(mmf)\n",
    "motion_model = MotionNet(config[\"spatiotemporal_encoding\"], \n",
    "                         config[\"spatiotemporal_network\"],\n",
    "                         config[\"deltaspatial_encoding\"], \n",
    "                         config[\"deltaspatial_network\"])\n",
    "motion_model.load_state_dict(torch.load(args.motion_model[\"load_path\"])[\"model_state_dict\"])\n",
    "# Freeze the model\n",
    "motion_model.eval()\n",
    "# for params in motion_model.parameters():\n",
    "#     params.requires_grad = False\n",
    "# Set the model device\n",
    "motion_spatiotemporal_device = torch.device(torch.device(\"cuda:0\"))\n",
    "motion_deltaspatial_device = torch.device(torch.device(\"cuda:0\"))\n",
    "motion_model.set_device(motion_spatiotemporal_device, motion_deltaspatial_device)\n",
    "\n",
    "# motion_tensor, _ = motion_model(dset.loc)\n",
    "# motion_tensor = motion_tensor.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folders for the traced video data and checkpoints.\n",
    "if args.append_save_path is not None:\n",
    "    args.trace[\"folder\"] =  args.trace[\"folder\"] + args.append_save_path\n",
    "    args.checkpoints[\"dir\"] =  args.checkpoints[\"dir\"] + args.append_save_path\n",
    "if args.trace[\"folder\"] is not None:\n",
    "    os.makedirs(args.trace[\"folder\"], exist_ok=True)\n",
    "    if args.verbose: print(f'Saving trace to {args.trace[\"folder\"]}')\n",
    "if args.checkpoints[\"save\"]:\n",
    "    os.makedirs(args.checkpoints[\"dir\"], exist_ok=True)\n",
    "    if args.verbose: print(f'Saving checkpoints to {args.checkpoints[\"dir\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info before iterating.\n",
    "epochs = args.train[\"epochs\"]\n",
    "ndigits_epoch = int(np.log10(epochs)+1)\n",
    "latest_ckpt_path = os.path.join(args.checkpoints[\"dir\"], args.checkpoints[\"latest\"])\n",
    "if os.path.exists(latest_ckpt_path):\n",
    "    if args.verbose: print('Loading latest checkpoint...')\n",
    "    # saved_dict = torch.load(latest_ckpt_path)\n",
    "    # pleth_model.load_state_dict(saved_dict[\"model_state_dict\"])\n",
    "    # if \"optimizer_spatial_state_dict\" in saved_dict.keys():\n",
    "    #     opt_spatial.load_state_dict(saved_dict[\"optimizer_spatial_state_dict\"])\n",
    "    # if \"optimizer_temporal_state_dict\" in saved_dict.keys():\n",
    "    #     opt_temporal.load_state_dict(saved_dict[\"optimizer_temporal_state_dict\"])\n",
    "    # start_epoch = saved_dict[\"epoch\"] + 1\n",
    "    # if args.verbose: print(f'Continuing from epoch {start_epoch}.')\n",
    "else:\n",
    "    if args.verbose: print('Starting from scratch.')\n",
    "    start_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "def pulse_rate_from_power_spectral_density(pleth_sig: np.array, FS: float,\n",
    "                                           LL_PR: float, UL_PR: float,\n",
    "                                           BUTTER_ORDER: int = 6,\n",
    "                                           DETREND: bool = False,\n",
    "                                           FResBPM: float = 0.1) -> float:\n",
    "    \"\"\" Function to estimate the pulse rate from the power spectral density of the plethysmography signal.\n",
    "    Args:\n",
    "        pleth_sig (np.array): Plethysmography signal.\n",
    "        FS (float): Sampling frequency.\n",
    "        LL_PR (float): Lower cutoff frequency for the butterworth filtering.\n",
    "        UL_PR (float): Upper cutoff frequency for the butterworth filtering.\n",
    "        BUTTER_ORDER (int, optional): Order of the butterworth filter. Give None to skip filtering. Defaults to 6.\n",
    "        DETREND (bool, optional): Boolena Flag for executing cutsom_detrend. Defaults to False.\n",
    "        FResBPM (float, optional): Frequency resolution. Defaults to 0.1.\n",
    "    Returns:\n",
    "        pulse_rate (float): _description_\n",
    "    \n",
    "    Daniel McDuff, Ethan Blackford, January 2019\n",
    "    Copyright (c)\n",
    "    Licensed under the MIT License and the RAIL AI License.\n",
    "    \"\"\"\n",
    "\n",
    "    N = (60*FS)/FResBPM\n",
    "\n",
    "    # Detrending + nth order butterworth + periodogram\n",
    "    if DETREND:\n",
    "        pleth_sig = custom_detrend(np.cumsum(pleth_sig), 100)\n",
    "    if BUTTER_ORDER:\n",
    "        [b, a] = scipy.signal.butter(BUTTER_ORDER, [LL_PR/60, UL_PR/60], btype='bandpass', fs = FS)\n",
    "    \n",
    "    pleth_sig = scipy.signal.filtfilt(b, a, np.double(pleth_sig))\n",
    "    \n",
    "    # Calculate the PSD and the mask for the desired range\n",
    "    F, Pxx = scipy.signal.periodogram(x=pleth_sig,  nfft=N, fs=FS);  \n",
    "    FMask = (F >= (LL_PR/60)) & (F <= (UL_PR/60))\n",
    "    \n",
    "    # Calculate predicted pulse rate:\n",
    "    FRange = F * FMask\n",
    "    PRange = Pxx * FMask\n",
    "    MaxInd = np.argmax(PRange)\n",
    "    pulse_rate_freq = FRange[MaxInd]\n",
    "    pulse_rate = pulse_rate_freq*60\n",
    "            \n",
    "    return pulse_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = VideoGridDataset(args.video_path, verbose=args.verbose, num_frames=args.data[\"num_frames\"], \n",
    "                        start_frame=args.data[\"start_frame\"], pixel_norm=args.data[\"norm_value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "# motion_tensor, _ = motion_model(dset.loc)\n",
    "# motion_tensor = motion_tensor.reshape(dset.shape).to(args.io_device)\n",
    "# motion_orig = deepcopy(motion_tensor.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (128,128,300,3)\n",
    "X,Y = np.meshgrid(np.arange(shape[1]),np.arange(shape[0]))\n",
    "T = np.arange(shape[2])\n",
    "X = (torch.tensor(X.ravel()) / shape[1]) - 0.5\n",
    "Y = (torch.tensor(Y.ravel()) / shape[0]) - 0.5\n",
    "T = (torch.tensor(T.ravel()) / shape[2]) - 0.5\n",
    "XY = torch.stack((X,Y), dim=-1)\n",
    "T0 = torch.stack((T,torch.zeros_like(T)), dim=-1)\n",
    "# pixel = dset.vid.reshape(shape)\n",
    "# pixel = dset.vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader = torch.utils.data.DataLoader(range(len(dset)), batch_size=args.data[\"batch_size\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pte = tcnn.Encoding(args.pleth_temporal_encoding[\"input_dims\"], args.pleth_temporal_encoding)\n",
    "# ptn = tcnn.Network(pte.n_output_dims, args.pleth_temporal_network[\"output_dims\"], args.pleth_temporal_network)\n",
    "# pleth_temporal_model = torch.nn.Sequential(pte,ptn)\n",
    "# pleth_temporal_model.to(args.io_device)\n",
    "# pleth_spatial_model = tcnn.NetworkWithInputEncoding(args.pleth_spatial_encoding[\"input_dims\"],\n",
    "#                                                     args.pleth_spatial_network[\"output_dims\"],\n",
    "#                                                     args.pleth_spatial_encoding,\n",
    "#                                                     args.pleth_spatial_network)\n",
    "# # ckpt = torch.load(os.path.join(args.pre_train_checkpoints[\"dir\"], args.pre_train_checkpoints[\"latest\"]))\n",
    "# # pleth_spatial_model.load_state_dict(ckpt['model_state_dict'])\n",
    "# pleth_spatial_model.to(args.io_device)\n",
    "# opt_temporal_enc = torch.optim.Adam(pte.parameters(), lr=args.opt[\"lr\"],\n",
    "#                                 betas=(args.opt[\"beta1\"], args.opt[\"beta2\"]), eps=args.opt[\"eps\"])\n",
    "# opt_temporal_net = torch.optim.Adam(ptn.parameters(), lr=args.opt[\"lr\"],\n",
    "#                                 betas=(args.opt[\"beta1\"], args.opt[\"beta2\"]), eps=args.opt[\"eps\"])\n",
    "# opt_spatial = torch.optim.Adam(pleth_spatial_model.parameters(), lr=args.opt[\"lr\"],\n",
    "#                                 betas=(args.opt[\"beta1\"], args.opt[\"beta2\"]), eps=args.opt[\"eps\"])\n",
    "\n",
    "# flag=False\n",
    "# switch = 3\n",
    "# for epoch in range(start_epoch,epochs+1):\n",
    "#     train_loss = 0\n",
    "#     pleth_temporal_model.train()\n",
    "#     pleth_spatial_model.train()\n",
    "#     motion_model.train()\n",
    "#     for count, item in tqdm(enumerate(dloader),total=len(dloader)):\n",
    "#         if epoch > switch:\n",
    "#             # print(\"Yes\")\n",
    "#             loc = dset.loc[item].half().to(args.io_device)\n",
    "#             pixel = dset.vid[item].half().to(args.io_device)\n",
    "#             XY_loc = loc[...,0:2]\n",
    "#             T_loc = torch.cat((loc[...,2:3], torch.zeros_like(loc[...,2:3])), dim=-1)\n",
    "#             motion_output, _ = motion_model(loc)\n",
    "#             pleth_spatial_output = pleth_spatial_model(XY_loc)\n",
    "#             pleth_temporal_output = pleth_temporal_model(T_loc)\n",
    "#             output = motion_output + (pleth_spatial_output*pleth_temporal_output)\n",
    "#             # Since the model takes care of moving the data to different devices, move GT correspondingly.\n",
    "#             pixel = pixel.to(output.dtype).to(output.device)\n",
    "#             # Backpropagation.\n",
    "#             if flag:\n",
    "#                 opt_temporal_enc.zero_grad()\n",
    "#                 opt_temporal_net.zero_grad()\n",
    "#                 l2_error = (output - pixel)**2\n",
    "#                 loss = l2_error.mean()\n",
    "#                 loss.backward()\n",
    "#                 opt_temporal_enc.step()\n",
    "#                 opt_temporal_net.step()\n",
    "#             else:\n",
    "#                 opt_spatial.zero_grad()\n",
    "#                 l2_error = (output - pixel)**2\n",
    "#                 loss = l2_error.mean()\n",
    "#                 loss.backward()\n",
    "#                 opt_spatial.step()\n",
    "#             train_loss += loss.item()\n",
    "#             if epoch % 1:\n",
    "#                 flag = not flag\n",
    "#             pass\n",
    "#         else:\n",
    "#             # print(\"No\")\n",
    "#             loc = dset.loc[item].half().to(args.io_device)\n",
    "#             pixel = dset.vid[item].half().to(args.io_device)\n",
    "#             T_loc = torch.cat((loc[...,2:3], torch.zeros_like(loc[...,2:3])), dim=-1)\n",
    "#             motion_output, _ = motion_model(loc)\n",
    "#             # pleth_output = pleth_model(loc)\n",
    "#             pleth_output = pleth_temporal_model(T_loc)\n",
    "#             output = motion_output + pleth_output\n",
    "#             # Since the model takes care of moving the data to different devices, move GT correspondingly.\n",
    "#             pixel = pixel.to(output.dtype).to(output.device)\n",
    "#             # Backpropagation.\n",
    "#             opt_temporal_enc.zero_grad()\n",
    "#             opt_temporal_net.zero_grad()\n",
    "#             l2_error = (output - pixel)**2\n",
    "#             loss = l2_error.mean()\n",
    "#             loss.backward()\n",
    "#             opt_temporal_enc.step()\n",
    "#             opt_temporal_net.step()\n",
    "#             train_loss += loss.item()\n",
    "\n",
    "\n",
    "#     print(f'Epoch: {epoch}, Loss: {train_loss/len(dloader)}', flush=True)\n",
    "#     with torch.no_grad():\n",
    "#         if epoch > switch:\n",
    "#             motion_model.eval()\n",
    "#             pleth_spatial_model.eval()\n",
    "#             pleth_temporal_model.eval()\n",
    "\n",
    "#             trace_loc = dset.loc.half().to(args.io_device)\n",
    "#             trace_motion, _ = motion_model(trace_loc)\n",
    "#             trace_XY_loc = trace_loc[...,0:2]\n",
    "#             trace_pleth_spatial = pleth_spatial_model(trace_XY_loc)\n",
    "#             trace_T_loc = torch.cat((trace_loc[...,2:3], torch.zeros_like(trace_loc[...,2:3])), dim=-1)\n",
    "#             trace_pleth_temporal = pleth_temporal_model(trace_T_loc)\n",
    "            \n",
    "#             trace = trace_motion + (trace_pleth_spatial*trace_pleth_temporal)\n",
    "#             trace = trace.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()\n",
    "#             trace = (np.clip(trace, 0, 1)*255).astype(np.uint8)\n",
    "#             save_path = os.path.join(args.trace[\"folder\"], f'{args.trace[\"file_tag\"]}{str(epoch).zfill(ndigits_epoch)}.avi')\n",
    "#             iio2.mimwrite(save_path, trace, fps=30)\n",
    "            \n",
    "#             trace = trace_motion.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()\n",
    "#             trace = (np.clip(trace, 0, 1)*255).astype(np.uint8)\n",
    "#             save_path = os.path.join(args.trace[\"folder\"], f'motion_{args.trace[\"file_tag\"]}{str(epoch).zfill(ndigits_epoch)}.avi')\n",
    "#             iio2.mimwrite(save_path, trace, fps=30)\n",
    "            \n",
    "#             trace = trace_pleth_spatial.detach().cpu().float().reshape(dset.shape[:3]).permute(2,0,1).numpy()\n",
    "#             trace = (np.clip(trace, 0, 1)*255).astype(np.uint8)\n",
    "#             save_path = os.path.join(args.trace[\"folder\"], f'mask_{args.trace[\"file_tag\"]}{str(epoch).zfill(ndigits_epoch)}.avi')\n",
    "#             iio2.mimwrite(save_path, trace, fps=30)\n",
    "#             pass\n",
    "#         else:\n",
    "#             motion_model.eval()\n",
    "#             pleth_temporal_model.eval()\n",
    "\n",
    "#             trace_loc = dset.loc.half().to(args.io_device)\n",
    "#             motion_output, _ = motion_model(trace_loc)\n",
    "\n",
    "#             trace_T_loc = torch.cat((trace_loc[...,2:3], torch.zeros_like(trace_loc[...,2:3])), dim=-1)\n",
    "#             pleth_output = pleth_temporal_model(trace_T_loc)\n",
    "\n",
    "#             trace = motion_output + pleth_output\n",
    "\n",
    "#             trace = trace.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()\n",
    "#             trace = (np.clip(trace, 0, 1)*255).astype(np.uint8)\n",
    "#             save_path = os.path.join(args.trace[\"folder\"], f'{args.trace[\"file_tag\"]}{str(epoch).zfill(ndigits_epoch)}.avi')\n",
    "#             iio2.mimwrite(save_path, trace, fps=30)\n",
    "            \n",
    "#             trace = motion_output.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()\n",
    "#             trace = (np.clip(trace, 0, 1)*255).astype(np.uint8)\n",
    "#             save_path = os.path.join(args.trace[\"folder\"], f'motion_{args.trace[\"file_tag\"]}{str(epoch).zfill(ndigits_epoch)}.avi')\n",
    "#             iio2.mimwrite(save_path, trace, fps=30)\n",
    "            \n",
    "#             # trace = pleth_output.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()\n",
    "#             # trace = (np.clip(trace, 0, 1)*255).astype(np.uint8)\n",
    "#             # save_path = os.path.join(args.trace[\"folder\"], f'pleth_{args.trace[\"file_tag\"]}{str(epoch).zfill(ndigits_epoch)}.avi')\n",
    "#             # iio2.mimwrite(save_path, trace, fps=30)\n",
    "            \n",
    "#             # trace = pleth_output.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()\n",
    "#             # trace = (trace - np.amin(trace, axis=(0,1,2), keepdims=True)) / (np.amax(trace, axis=(0,1,2), keepdims=True) - np.amin(trace, axis=(0,1,2), keepdims=True))\n",
    "#             # trace = trace/20 + 0.5\n",
    "#             # trace = (np.clip(trace, 0, 1)*255).astype(np.uint8)\n",
    "#             # save_path = os.path.join(args.trace[\"folder\"], f'pleth_scaled_{args.trace[\"file_tag\"]}{str(epoch).zfill(ndigits_epoch)}.avi')\n",
    "#             # iio2.mimwrite(save_path, trace, fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100%|██████████| 600/600 [00:12<00:00, 47.74it/s]Epoch: 1, Loss: 0.00011248618364334106\n",
    "\n",
    "# 100%|██████████| 600/600 [00:12<00:00, 49.90it/s]Epoch: 2, Loss: 5.8252314726511634e-05\n",
    "\n",
    "# 100%|██████████| 600/600 [00:12<00:00, 47.62it/s]Epoch: 3, Loss: 6.430208683013916e-05\n",
    "\n",
    "# 100%|██████████| 600/600 [00:12<00:00, 46.75it/s]Epoch: 4, Loss: 4.203597704569499e-05\n",
    "\n",
    "# 100%|██████████| 600/600 [00:12<00:00, 48.58it/s]Epoch: 5, Loss: 4.323959350585938e-05\n",
    "\n",
    "# 100%|██████████| 600/600 [00:12<00:00, 47.33it/s]Epoch: 6, Loss: 4.204720258712768e-05\n",
    "\n",
    "# 100%|██████████| 600/600 [00:12<00:00, 47.14it/s]Epoch: 7, Loss: 4.2064587275187176e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pleth_temporal_model = tcnn.NetworkWithInputEncoding(args.pleth_temporal_encoding[\"input_dims\"],\n",
    "#                                                      args.pleth_temporal_network[\"output_dims\"],\n",
    "#                                                      args.pleth_temporal_encoding,\n",
    "#                                                      args.pleth_temporal_network)\n",
    "# pleth_temporal_model.to(args.io_device)\n",
    "# pleth_spatial_model = tcnn.NetworkWithInputEncoding(args.pleth_spatial_encoding[\"input_dims\"],\n",
    "#                                                     args.pleth_spatial_network[\"output_dims\"],\n",
    "#                                                     args.pleth_spatial_encoding,\n",
    "#                                                     args.pleth_spatial_network)\n",
    "# ckpt = torch.load(os.path.join(args.pre_train_checkpoints[\"dir\"], args.pre_train_checkpoints[\"latest\"]))\n",
    "# pleth_spatial_model.load_state_dict(ckpt['model_state_dict'])\n",
    "# pleth_spatial_model.to(args.io_device)\n",
    "# opt_temporal = torch.optim.Adam(pleth_temporal_model.parameters(), lr=args.opt[\"lr\"],\n",
    "#                                 betas=(args.opt[\"beta1\"], args.opt[\"beta2\"]), eps=args.opt[\"eps\"])\n",
    "# opt_spatial = torch.optim.Adam(pleth_spatial_model.parameters(), lr=args.opt[\"lr\"],\n",
    "#                                 betas=(args.opt[\"beta1\"], args.opt[\"beta2\"]), eps=args.opt[\"eps\"])\n",
    "\n",
    "# flag=True\n",
    "# for epoch in range(start_epoch,epochs+1):\n",
    "#     train_loss = 0\n",
    "#     pleth_temporal_model.train()\n",
    "#     pleth_spatial_model.train()\n",
    "#     motion_model.train()\n",
    "#     for count, item in tqdm(enumerate(dloader),total=len(dloader)):\n",
    "#         loc = dset.loc[item].half().to(args.io_device)\n",
    "#         pixel = dset.vid[item].half().to(args.io_device)\n",
    "#         XY_loc = loc[...,0:2]\n",
    "#         T_loc = torch.cat((loc[...,2:3], torch.zeros_like(loc[...,2:3])), dim=-1)\n",
    "#         motion_output, _ = motion_model(loc)\n",
    "#         pleth_spatial_output = pleth_spatial_model(XY_loc)\n",
    "#         pleth_temporal_output = pleth_temporal_model(T_loc)\n",
    "#         output = motion_output + (pleth_spatial_output*pleth_temporal_output)\n",
    "#         # Since the model takes care of moving the data to different devices, move GT correspondingly.\n",
    "#         pixel = pixel.to(output.dtype).to(output.device)\n",
    "#         # Backpropagation.\n",
    "#         if flag:\n",
    "#             opt_temporal.zero_grad()\n",
    "#             l2_error = (output - pixel)**2\n",
    "#             loss = l2_error.mean()\n",
    "#             loss.backward()\n",
    "#             opt_temporal.step()\n",
    "#         else:\n",
    "#             opt_spatial.zero_grad()\n",
    "#             l2_error = (output - pixel)**2\n",
    "#             loss = l2_error.mean()\n",
    "#             loss.backward()\n",
    "#             opt_spatial.step()\n",
    "#         train_loss += loss.item()\n",
    "#         if epoch % 1:\n",
    "#             flag = not flag\n",
    "#     print(f'Epoch: {epoch}, Loss: {train_loss/len(dloader)}', flush=True)\n",
    "#     with torch.no_grad():\n",
    "#         motion_model.eval()\n",
    "#         pleth_spatial_model.eval()\n",
    "#         pleth_temporal_model.eval()\n",
    "\n",
    "#         trace_loc = dset.loc.half().to(args.io_device)\n",
    "#         trace_motion, _ = motion_model(trace_loc)\n",
    "#         trace_XY_loc = trace_loc[...,0:2]\n",
    "#         trace_pleth_spatial = pleth_spatial_model(trace_XY_loc)\n",
    "#         trace_T_loc = torch.cat((trace_loc[...,2:3], torch.zeros_like(trace_loc[...,2:3])), dim=-1)\n",
    "#         trace_pleth_temporal = pleth_temporal_model(trace_T_loc)\n",
    "        \n",
    "#         trace = trace_motion + (trace_pleth_spatial*trace_pleth_temporal)\n",
    "#         trace = trace.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()\n",
    "#         trace = (np.clip(trace, 0, 1)*255).astype(np.uint8)\n",
    "#         save_path = os.path.join(args.trace[\"folder\"], f'{args.trace[\"file_tag\"]}{str(epoch).zfill(ndigits_epoch)}.avi')\n",
    "#         iio2.mimwrite(save_path, trace, fps=30)\n",
    "        \n",
    "#         trace = trace_motion.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()\n",
    "#         trace = (np.clip(trace, 0, 1)*255).astype(np.uint8)\n",
    "#         save_path = os.path.join(args.trace[\"folder\"], f'motion_{args.trace[\"file_tag\"]}{str(epoch).zfill(ndigits_epoch)}.avi')\n",
    "#         iio2.mimwrite(save_path, trace, fps=30)\n",
    "        \n",
    "#         trace = trace_pleth_spatial.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()\n",
    "#         trace = (np.clip(trace, 0, 1)*255).astype(np.uint8)\n",
    "#         save_path = os.path.join(args.trace[\"folder\"], f'mask_{args.trace[\"file_tag\"]}{str(epoch).zfill(ndigits_epoch)}.avi')\n",
    "#         iio2.mimwrite(save_path, trace, fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pleth_model = tcnn.NetworkWithInputEncoding(args.pleth_temporal_encoding[\"input_dims\"],\n",
    "#                                             args.pleth_temporal_network[\"output_dims\"],\n",
    "#                                             args.pleth_temporal_encoding,\n",
    "#                                             args.pleth_temporal_network, seed=sd)\n",
    "# pleth_model.to(args.io_device)\n",
    "# opt = torch.optim.Adam(pleth_model.parameters(), lr=args.opt[\"lr\"],\n",
    "#                        betas=(args.opt[\"beta1\"], args.opt[\"beta2\"]), eps=args.opt[\"eps\"])\n",
    "\n",
    "# for epoch in range(start_epoch,epochs+1):\n",
    "#     train_loss = 0\n",
    "#     pleth_model.train()\n",
    "#     motion_model.train()\n",
    "#     for count, item in tqdm(enumerate(dloader),total=len(dloader)):\n",
    "#         loc = dset.loc[item].half().to(args.io_device)\n",
    "#         pixel = dset.vid[item].half().to(args.io_device)\n",
    "#         T_loc = torch.cat((loc[...,2:3], torch.zeros_like(loc[...,2:3])), dim=-1)\n",
    "#         motion_output, _ = motion_model(loc)\n",
    "#         # pleth_output = pleth_model(loc)\n",
    "#         pleth_output = pleth_model(T_loc)\n",
    "#         output = motion_output + pleth_output\n",
    "#         # Since the model takes care of moving the data to different devices, move GT correspondingly.\n",
    "#         pixel = pixel.to(output.dtype).to(output.device)\n",
    "#         # Backpropagation.\n",
    "#         opt.zero_grad()\n",
    "#         l2_error = (output - pixel)**2\n",
    "#         loss = l2_error.mean()\n",
    "#         loss.backward()\n",
    "#         opt.step()\n",
    "#         train_loss += loss.item()\n",
    "#     print(f'Epoch: {epoch}, Loss: {train_loss/len(dloader)}', flush=True)\n",
    "#     # with torch.no_grad():\n",
    "#     #     motion_model.eval()\n",
    "#     #     pleth_model.eval()\n",
    "#     #     trace_loc = dset.loc.half().to(args.io_device)\n",
    "#     #     trace_T_loc = torch.cat((trace_loc[...,2:3], torch.zeros_like(trace_loc[...,2:3])), dim=-1)\n",
    "#     #     motion_output, _ = motion_model(trace_loc)\n",
    "#     #     # pleth_output = pleth_model(trace_loc)\n",
    "#     #     pleth_output = pleth_model(trace_T_loc)\n",
    "#     #     trace = motion_output + pleth_output\n",
    "#     #     trace = trace.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()\n",
    "#     #     trace = (np.clip(trace, 0, 1)*255).astype(np.uint8)\n",
    "#     #     save_path = os.path.join(args.trace[\"folder\"], f'{args.trace[\"file_tag\"]}{str(epoch).zfill(ndigits_epoch)}.avi')\n",
    "#     #     iio2.mimwrite(save_path, trace, fps=30)\n",
    "#     #     trace = motion_output.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()\n",
    "#     #     trace = (np.clip(trace, 0, 1)*255).astype(np.uint8)\n",
    "#     #     save_path = os.path.join(args.trace[\"folder\"], f'motion_{args.trace[\"file_tag\"]}{str(epoch).zfill(ndigits_epoch)}.avi')\n",
    "#     #     iio2.mimwrite(save_path, trace, fps=30)\n",
    "#     #     trace = pleth_output.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()\n",
    "#     #     trace = (np.clip(trace, 0, 1)*255).astype(np.uint8)\n",
    "#     #     save_path = os.path.join(args.trace[\"folder\"], f'pleth_{args.trace[\"file_tag\"]}{str(epoch).zfill(ndigits_epoch)}.avi')\n",
    "#     #     iio2.mimwrite(save_path, trace, fps=30)\n",
    "#     #     trace = pleth_output.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()\n",
    "#     #     if not np.array_equal(np.amax(trace, axis=(1,2), keepdims=True), np.amin(trace, axis=(1,2), keepdims=True)):\n",
    "#     #         trace = (trace - np.amin(trace, axis=(1,2), keepdims=True)) / (np.amax(trace, axis=(1,2), keepdims=True) - np.amin(trace, axis=(1,2), keepdims=True))\n",
    "#     #     else:\n",
    "#     #         trace = (trace - np.amin(trace, axis=(0,1,2), keepdims=True)) / (np.amax(trace, axis=(0,1,2), keepdims=True) - np.amin(trace, axis=(0,1,2), keepdims=True))\n",
    "#     #         trace = trace/20 + 0.5\n",
    "#     #     trace = (np.clip(trace, 0, 1)*255).astype(np.uint8)\n",
    "#     #     save_path = os.path.join(args.trace[\"folder\"], f'pleth_scaled_{args.trace[\"file_tag\"]}{str(epoch).zfill(ndigits_epoch)}.avi')\n",
    "#     #     iio2.mimwrite(save_path, trace, fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pleth_encoding_config = {\n",
    "    \"otype\": \"HashGrid\",\n",
    "    \"input_dims\": 2,\n",
    "    \"n_levels\": 8,\n",
    "    \"n_features_per_level\": 2,\n",
    "    \"log2_hashmap_size\": 24,\n",
    "    \"base_resolution\": 16,\n",
    "    \"per_level_scale\": 1.5\n",
    "}\n",
    "pleth_network_config = {\n",
    "    \"otype\": \"CutlassMLP\",\n",
    "    \"activation\": \"Sine\",\n",
    "    \"output_activation\": \"none\",\n",
    "    \"n_neurons\": 64,\n",
    "    \"n_hidden_layers\": 2,\n",
    "    \"output_dims\": 3\n",
    "}\n",
    "pleth_enc = tcnn.Encoding(3, pleth_encoding_config)\n",
    "pleth_net = tcnn.Network(pleth_enc.n_output_dims, 3, pleth_network_config)\n",
    "pleth_model = torch.nn.Sequential(pleth_enc, pleth_net)\n",
    "pleth_model.to(args.io_device)\n",
    "opt_enc = torch.optim.Adam(pleth_enc.parameters(), lr=1e-3,\n",
    "                       betas=(args.opt[\"beta1\"], args.opt[\"beta2\"]), eps=args.opt[\"eps\"])\n",
    "opt_net = torch.optim.Adam(pleth_net.parameters(), lr=1e-3, weight_decay=1e-6,\n",
    "                       betas=(args.opt[\"beta1\"], args.opt[\"beta2\"]), eps=args.opt[\"eps\"])\n",
    "epochs = 10\n",
    "for epoch in range(start_epoch,epochs+1):\n",
    "    train_loss = 0\n",
    "    pleth_model.train()\n",
    "    motion_model.train()\n",
    "    for count, item in tqdm(enumerate(dloader),total=len(dloader)):\n",
    "        loc = dset.loc[item].half().to(args.io_device)\n",
    "        pixel = dset.vid[item].half().to(args.io_device)\n",
    "        motion_output, _ = motion_model(loc)\n",
    "        pleth_output = pleth_model(loc)\n",
    "        output = motion_output + pleth_output\n",
    "        # Since the model takes care of moving the data to different devices, move GT correspondingly.\n",
    "        pixel = pixel.to(output.dtype).to(output.device)\n",
    "        # Backpropagation.\n",
    "        opt_enc.zero_grad()\n",
    "        opt_net.zero_grad()\n",
    "        l2_error = (output - pixel)**2\n",
    "        loss = l2_error.mean()\n",
    "        loss.backward()\n",
    "        opt_enc.step()\n",
    "        opt_net.step()\n",
    "        train_loss += loss.item()\n",
    "    print(f'Epoch: {epoch}, Loss: {train_loss/len(dloader)}', flush=True)\n",
    "    with torch.no_grad():\n",
    "        motion_model.eval()\n",
    "        pleth_model.eval()\n",
    "        trace_loc = dset.loc.half().to(args.io_device)\n",
    "        motion_output, _ = motion_model(trace_loc)\n",
    "        pleth_output = pleth_model(trace_loc)\n",
    "        \n",
    "        trace = motion_output + pleth_output\n",
    "        trace = trace.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()\n",
    "        trace = (np.clip(trace, 0, 1)*255).astype(np.uint8)\n",
    "        save_path = os.path.join(args.trace[\"folder\"], f'{args.trace[\"file_tag\"]}{str(epoch).zfill(ndigits_epoch)}.avi')\n",
    "        iio2.mimwrite(save_path, trace, fps=30)\n",
    "        \n",
    "        trace = pleth_output.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()\n",
    "        trace = (trace - np.amin(trace, axis=(0,1,2), keepdims=True)) / (np.amax(trace, axis=(0,1,2), keepdims=True) - np.amin(trace, axis=(0,1,2), keepdims=True))\n",
    "        trace = (np.clip(trace, 0, 1)*255).astype(np.uint8)\n",
    "        save_path = os.path.join(args.trace[\"folder\"], f'rescaled_residual_{args.trace[\"file_tag\"]}{str(epoch).zfill(ndigits_epoch)}.avi')\n",
    "        iio2.mimwrite(save_path, trace, fps=30)\n",
    "        \n",
    "        trace = motion_output.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()\n",
    "        trace = (np.clip(trace, 0, 1)*255).astype(np.uint8)\n",
    "        save_path = os.path.join(args.trace[\"folder\"], f'motion_{args.trace[\"file_tag\"]}{str(epoch).zfill(ndigits_epoch)}.avi')\n",
    "        iio2.mimwrite(save_path, trace, fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_model.eval()\n",
    "pleth_model.eval()\n",
    "trace_loc = dset.loc.half().to(args.io_device)\n",
    "motion_output, _ = motion_model(trace_loc)\n",
    "pleth_output = pleth_model(trace_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_trace = motion_output.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()\n",
    "pleth_trace  =  pleth_output.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_frame = [motion_trace[0]]\n",
    "rep_motion_trace = np.stack(motion_frame*300, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_with_xyt = rep_motion_trace + pleth_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = (np.clip(static_with_xyt, 0, 1)*255).astype(np.uint8)\n",
    "iio2.mimwrite('temp.avi', trace, fps=30)\n",
    "trace.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hashppg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b310efec2753918fc2ffe1b7b62e4629d4af86145c6b3c505a1f8291347894dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
