{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import commentjson as json\n",
    "import imageio.v2 as iio2\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import tinycudann as tcnn\n",
    "import argparse\n",
    "\n",
    "from implicitpleth.models.siren import Siren\n",
    "from implicitpleth.models.combinations import MotionNet\n",
    "from implicitpleth.data.datasets import VideoGridDataset\n",
    "from implicitpleth.utils.utils import trace_video, Dict2Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fix random seed\n",
    "# sd = 0\n",
    "# np.random.seed(sd)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.manual_seed(sd)\n",
    "# random.seed(sd)\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed_all(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./configs/residual.json\") as f:\n",
    "    json_config = json.load(f)\n",
    "\n",
    "_trial = 'v_99_2'\n",
    "json_config[\"video_path\"] = f'/home/pradyumnachari/Documents/ImplicitPPG/SIGGRAPH_Data/rgb_files/{_trial}'\n",
    "json_config[\"verbose\"] = True\n",
    "json_config[\"append_save_path\"] = None\n",
    "json_config[\"append_load_path\"] = None\n",
    "\n",
    "json_config[\"motion_model\"][\"load_path\"] = f'delta_motion/motion_model.pth'\n",
    "json_config[\"motion_model\"][\"load_path\"] = f'/home/pradyumnachari/Documents/FastImplicitPleth/dataset_motion/motion_0_{_trial}/epoch_10.pth'\n",
    "args = Dict2Class(json_config)\n",
    "print(args.spatiotemporal_device, args.deltaspatial_device, args.pleth_device)\n",
    "args.spatiotemporal_device = torch.device(args.spatiotemporal_device)\n",
    "args.deltaspatial_device = torch.device(args.deltaspatial_device)\n",
    "args.pleth_device = torch.device(args.pleth_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(args.motion_model)\n",
    "with open(args.motion_model[\"config\"]) as mmf:\n",
    "    config = json.load(mmf)\n",
    "motion_model = MotionNet(config[\"spatiotemporal_encoding\"], \n",
    "                         config[\"spatiotemporal_network\"],\n",
    "                         config[\"deltaspatial_encoding\"], \n",
    "                         config[\"deltaspatial_network\"])\n",
    "motion_model.load_state_dict(torch.load(args.motion_model[\"load_path\"])[\"model_state_dict\"])\n",
    "# Freeze the model\n",
    "motion_model.eval()\n",
    "# for params in motion_model.parameters():\n",
    "#     params.requires_grad = False\n",
    "# Set the model device\n",
    "motion_spatiotemporal_device = args.spatiotemporal_device\n",
    "motion_deltaspatial_device = args.deltaspatial_device\n",
    "motion_model.set_device(motion_spatiotemporal_device, motion_deltaspatial_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folders for the traced video data and checkpoints.\n",
    "if args.append_save_path is not None:\n",
    "    args.trace[\"folder\"] =  args.trace[\"folder\"] + args.append_save_path\n",
    "    args.checkpoints[\"dir\"] =  args.checkpoints[\"dir\"] + args.append_save_path\n",
    "if args.trace[\"folder\"] is not None:\n",
    "    os.makedirs(args.trace[\"folder\"], exist_ok=True)\n",
    "    if args.verbose: print(f'Saving trace to {args.trace[\"folder\"]}')\n",
    "if args.checkpoints[\"save\"]:\n",
    "    os.makedirs(args.checkpoints[\"dir\"], exist_ok=True)\n",
    "    if args.verbose: print(f'Saving checkpoints to {args.checkpoints[\"dir\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info before iterating.\n",
    "epochs = args.train[\"epochs\"]\n",
    "ndigits_epoch = int(np.log10(epochs)+1)\n",
    "latest_ckpt_path = os.path.join(args.checkpoints[\"dir\"], args.checkpoints[\"latest\"])\n",
    "if os.path.exists(latest_ckpt_path):\n",
    "    if args.verbose: print('Loading latest checkpoint...')\n",
    "    # saved_dict = torch.load(latest_ckpt_path)\n",
    "    # pleth_model.load_state_dict(saved_dict[\"model_state_dict\"])\n",
    "    # if \"optimizer_spatial_state_dict\" in saved_dict.keys():\n",
    "    #     opt_spatial.load_state_dict(saved_dict[\"optimizer_spatial_state_dict\"])\n",
    "    # if \"optimizer_temporal_state_dict\" in saved_dict.keys():\n",
    "    #     opt_temporal.load_state_dict(saved_dict[\"optimizer_temporal_state_dict\"])\n",
    "    # start_epoch = saved_dict[\"epoch\"] + 1\n",
    "    # if args.verbose: print(f'Continuing from epoch {start_epoch}.')\n",
    "else:\n",
    "    if args.verbose: print('Starting from scratch.')\n",
    "    start_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "# motion_tensor, _ = motion_model(dset.loc)\n",
    "# motion_tensor = motion_tensor.reshape(dset.shape).to(args.pleth_device)\n",
    "# motion_orig = deepcopy(motion_tensor.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = VideoGridDataset(args.video_path, verbose=args.verbose, num_frames=args.data[\"num_frames\"], \n",
    "                        start_frame=args.data[\"start_frame\"], pixel_norm=args.data[\"norm_value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader = torch.utils.data.DataLoader(range(len(dset)), batch_size=args.data[\"batch_size\"], shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pleth_encoding_config = {\n",
    "    \"otype\": \"HashGrid\",\n",
    "    \"input_dims\": 3,\n",
    "    \"n_levels\": 8,\n",
    "    \"n_features_per_level\": 2,\n",
    "    \"log2_hashmap_size\": 24,\n",
    "    \"base_resolution\": 16,\n",
    "    \"per_level_scale\": 1.5\n",
    "}\n",
    "pleth_network_config = {\n",
    "    \"otype\": \"CutlassMLP\",\n",
    "    \"activation\": \"Sine\",\n",
    "    \"output_activation\": \"none\",\n",
    "    \"n_neurons\": 64,\n",
    "    \"n_hidden_layers\": 2,\n",
    "    \"output_dims\": 3\n",
    "}\n",
    "pleth_enc = tcnn.Encoding(pleth_encoding_config[\"input_dims\"], pleth_encoding_config)\n",
    "pleth_net = tcnn.Network(pleth_enc.n_output_dims, pleth_network_config[\"output_dims\"], pleth_network_config)\n",
    "pleth_model = torch.nn.Sequential(pleth_enc, pleth_net)\n",
    "pleth_model.to(args.pleth_device)\n",
    "lr = 1e-4\n",
    "opt_enc = torch.optim.Adam(pleth_enc.parameters(), lr=lr,\n",
    "                       betas=(args.opt[\"beta1\"], args.opt[\"beta2\"]), eps=args.opt[\"eps\"])\n",
    "opt_net = torch.optim.Adam(pleth_net.parameters(), lr=lr, weight_decay=1e-6,\n",
    "                       betas=(args.opt[\"beta1\"], args.opt[\"beta2\"]), eps=args.opt[\"eps\"])\n",
    "epochs = 10\n",
    "for epoch in range(start_epoch,epochs+1):\n",
    "    train_loss = 0\n",
    "    pleth_model.train()\n",
    "    motion_model.train()\n",
    "    for count, item in tqdm(enumerate(dloader),total=len(dloader)):\n",
    "        loc = dset.loc[item].half().to(args.pleth_device)\n",
    "        pixel = dset.vid[item].half().to(args.pleth_device)\n",
    "        motion_output, _ = motion_model(loc)\n",
    "        pleth_output = pleth_model(loc)\n",
    "        output = motion_output + pleth_output\n",
    "        # Since the model takes care of moving the data to different devices, move GT correspondingly.\n",
    "        pixel = pixel.to(output.dtype).to(output.device)\n",
    "        # Backpropagation.\n",
    "        opt_enc.zero_grad()\n",
    "        opt_net.zero_grad()\n",
    "        l2_error = (output - pixel)**2\n",
    "        loss = l2_error.mean()\n",
    "        loss.backward()\n",
    "        opt_enc.step()\n",
    "        opt_net.step()\n",
    "        train_loss += loss.item()\n",
    "    print(f'Epoch: {epoch}, Loss: {train_loss/len(dloader)}', flush=True)\n",
    "    with torch.no_grad():\n",
    "        motion_model.eval()\n",
    "        pleth_model.eval()\n",
    "        trace_loc = dset.loc.half().to(args.pleth_device)\n",
    "        motion_output, _ = motion_model(trace_loc)\n",
    "        pleth_output = pleth_model(trace_loc)\n",
    "        \n",
    "        trace = motion_output + pleth_output\n",
    "        trace = trace.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()\n",
    "        trace = (np.clip(trace, 0, 1)*255).astype(np.uint8)\n",
    "        save_path = os.path.join(args.trace[\"folder\"], f'{args.trace[\"file_tag\"]}{str(epoch).zfill(ndigits_epoch)}.avi')\n",
    "        iio2.mimwrite(save_path, trace, fps=30)\n",
    "        \n",
    "        trace = pleth_output.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()\n",
    "        trace = (trace - np.amin(trace, axis=(0,1,2), keepdims=True)) / (np.amax(trace, axis=(0,1,2), keepdims=True) - np.amin(trace, axis=(0,1,2), keepdims=True))\n",
    "        trace = (np.clip(trace, 0, 1)*255).astype(np.uint8)\n",
    "        save_path = os.path.join(args.trace[\"folder\"], f'rescaled_residual_{args.trace[\"file_tag\"]}{str(epoch).zfill(ndigits_epoch)}.avi')\n",
    "        iio2.mimwrite(save_path, trace, fps=30)\n",
    "        \n",
    "        trace = motion_output.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()\n",
    "        trace = (np.clip(trace, 0, 1)*255).astype(np.uint8)\n",
    "        save_path = os.path.join(args.trace[\"folder\"], f'motion_{args.trace[\"file_tag\"]}{str(epoch).zfill(ndigits_epoch)}.avi')\n",
    "        iio2.mimwrite(save_path, trace, fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_model.eval()\n",
    "pleth_model.eval()\n",
    "trace_loc = dset.loc.half().to(args.pleth_device)\n",
    "motion_output, _ = motion_model(trace_loc)\n",
    "pleth_output = pleth_model(trace_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_trace = motion_output.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()\n",
    "pleth_trace  =  pleth_output.detach().cpu().float().reshape(dset.shape).permute(2,0,1,3).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_frame = [motion_trace[0]]\n",
    "rep_motion_trace = np.stack(motion_frame*300, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_with_xyt = rep_motion_trace + pleth_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = (np.clip(static_with_xyt, 0, 1)*255).astype(np.uint8)\n",
    "iio2.mimwrite('temp.avi', trace, fps=30)\n",
    "trace.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = pleth_trace\n",
    "trace = (trace - np.amin(trace, axis=(0,1,2), keepdims=True)) / (np.amax(trace, axis=(0,1,2), keepdims=True) - np.amin(trace, axis=(0,1,2), keepdims=True))\n",
    "trace = (np.clip(trace, 0, 1)*255).astype(np.uint8)\n",
    "iio2.mimwrite('pleth.avi', trace, fps=30)\n",
    "trace.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hashppg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b310efec2753918fc2ffe1b7b62e4629d4af86145c6b3c505a1f8291347894dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
