{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2321,
     "status": "ok",
     "timestamp": 1672970717147,
     "user": {
      "displayName": "Pradyumna Chari",
      "userId": "06555723454452599039"
     },
     "user_tz": 480
    },
    "id": "IlPV8Ajddobz",
    "outputId": "9785a7e6-d9a2-40ed-8b9e-b0e91cab74a6"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "import scipy\n",
    "import random\n",
    "from utils import getErrors, CNN3D\n",
    "\n",
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))\n",
    "\n",
    "#seeds\n",
    "sd = 42\n",
    "# torch.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "\n",
    "np.random.seed(sd)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(sd)\n",
    "random.seed(sd)\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed_all(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVH-WL5kxA4f"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Code of 'Remote Photoplethysmograph Signal Measurement from Facial Videos Using Spatio-Temporal Networks' \n",
    "By Zitong Yu, 2019/05/05\n",
    "If you use the code, please cite:\n",
    "@inproceedings{yu2019remote,\n",
    "    title={Remote Photoplethysmograph Signal Measurement from Facial Videos Using Spatio-Temporal Networks},\n",
    "    author={Yu, Zitong and Li, Xiaobai and Zhao, Guoying},\n",
    "    booktitle= {British Machine Vision Conference (BMVC)},\n",
    "    year = {2019}\n",
    "}\n",
    "Only for research purpose, and commercial use is not allowed.\n",
    "MIT License\n",
    "Copyright (c) 2019 \n",
    "      How to use it\n",
    "    #1. Inference the model\n",
    "    rPPG, x_visual, x_visual3232, x_visual1616 = model(inputs)\n",
    "    \n",
    "    #2. Normalized the Predicted rPPG signal and GroundTruth BVP signal\n",
    "    rPPG = (rPPG-torch.mean(rPPG)) /torch.std(rPPG)\t \t# normalize\n",
    "    BVP_label = (BVP_label-torch.mean(BVP_label)) /torch.std(BVP_label)\t \t# normalize\n",
    "    \n",
    "    #3. Calculate the loss\n",
    "    loss_ecg = Neg_Pearson(rPPG, BVP_label)\n",
    "'''\n",
    "########################################\n",
    "\n",
    "\n",
    "class Neg_Pearson2(torch.nn.Module):    # Pearson range [-1, 1] so if < 0, abs|loss| ; if >0, 1- loss\n",
    "    def __init__(self):\n",
    "        super(Neg_Pearson2,self).__init__()\n",
    "\n",
    "        self.epsilon = 1e-2\n",
    "\n",
    "        return\n",
    "    def forward(self, preds, labels):       # tensor [Batch, Temporal]\n",
    "        loss = 0\n",
    "        for i in range(preds.shape[0]):\n",
    "            # print(labels[i])\n",
    "            # print(preds[i])\n",
    "            x = normalize_signal2(preds[i])\n",
    "            y = normalize_signal2(labels[i])\n",
    "\n",
    "            sum_x = torch.sum(x)                # x\n",
    "            sum_y = torch.sum(y)               # y\n",
    "            sum_xy = torch.sum(x*y)         # xy\n",
    "            sum_x2 = torch.sum(torch.pow(x,2))  # x^2\n",
    "            sum_y2 = torch.sum(torch.pow(y,2)) # y^2\n",
    "            N = preds.shape[1]\n",
    "            pearson = (N*sum_xy - sum_x*sum_y)/(torch.sqrt((N*sum_x2 - torch.pow(sum_x,2)+self.epsilon)*(N*sum_y2 - torch.pow(sum_y,2)+self.epsilon)))\n",
    "\n",
    "            # print(torch.sqrt((N*sum_x2 - torch.pow(sum_x,2))), (N*sum_y2 - torch.pow(sum_y,2)))\n",
    "            # print(pearson, end=\" \")\n",
    "            #if (pearson>=0).data.cpu().numpy():    # torch.cuda.ByteTensor -->  numpy\n",
    "            #    loss += 1 - pearson\n",
    "            #else:\n",
    "            #    loss += 1 - torch.abs(pearson)\n",
    "            \n",
    "            loss += (1 - pearson)**2\n",
    "            \n",
    "        # print(loss) \n",
    "        loss = loss/preds.shape[0]\n",
    "        return loss\n",
    "\n",
    "\n",
    "def normalize_signal2(sig):\n",
    "    return (sig-torch.mean(sig)) / (torch.std(sig)+1.00e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpboqDmlSGHS"
   },
   "outputs": [],
   "source": [
    "##UNIMODAL DATALOADER\n",
    "class RppgData2(Dataset):\n",
    "    def __init__(self, datapath, datapath2, datapaths, recording_str, video_length = 900, num_segments = 3, frame_length = 300, fs=30, l_freq_bpm=45, u_freq_bpm=180, fft_resolution = 48) -> None:\n",
    "        self.ppg_offset = 25\n",
    "        self.num_samps = 30\n",
    "\n",
    "        self.no_att_frms = 64\n",
    "        #Data structure for videos\n",
    "        self.video_length = video_length\n",
    "        self.num_segments = num_segments\n",
    "        self.part_length = int(video_length/num_segments)\n",
    "        self.datapath = datapath\n",
    "        self.datapath2 = datapath2\n",
    "\n",
    "        self.fs = fs\n",
    "        self.l_freq_bpm = l_freq_bpm\n",
    "        self.u_freq_bpm = u_freq_bpm\n",
    "        self.fft_resolution = fft_resolution\n",
    "        \n",
    "        #load videos and signals\n",
    "        self.video_list = datapaths\n",
    "        self.signal_list = []\n",
    "        self.item_list = []\n",
    "        #load signals\n",
    "        remove_folders = []\n",
    "        for folder in self.video_list:\n",
    "            file_path1 = os.path.join(self.datapath, folder)\n",
    "            file_path1_1 = os.path.join(self.datapath2, 'residual_0_'+folder+'.npy')\n",
    "            file_path2_1 = os.path.join(self.datapath2, 'residual_300_'+folder+'.npy')\n",
    "            file_path3_1 = os.path.join(self.datapath2, 'residual_600_'+folder+'.npy')\n",
    "\n",
    "            if (os.path.exists(file_path1) and os.path.exists(file_path1_1) and os.path.exists(file_path2_1) and os.path.exists(file_path3_1)):\n",
    "                if(os.path.exists(os.path.join(self.datapath,folder, f\"rgbd_ppg.npy\"))):\n",
    "                    signal = np.load(os.path.join(self.datapath,folder, f\"rgbd_ppg.npy\"))\n",
    "                    mean_temp = np.mean(signal)\n",
    "                    std_temp = np.std(signal)\n",
    "                    signal = (signal - mean_temp)/std_temp\n",
    "                    self.signal_list.append(signal[self.ppg_offset:])\n",
    "                else:\n",
    "                    print(folder, \"ppg doesn't exist.\")\n",
    "                    remove_folders.append(folder)\n",
    "            else:\n",
    "                print(folder, \" doesn't exist.\")\n",
    "                remove_folders.append(folder)\n",
    "\n",
    "        for i in remove_folders:\n",
    "            self.video_list.remove(i)    \n",
    "            print(\"removed\", i)\n",
    "            \n",
    "        self.signal_list = np.array(self.signal_list)\n",
    "\n",
    "        # Create a list of video number and valid frame nuber to extract the datad from.\n",
    "        self.frame_length = frame_length\n",
    "        self.video_nums = np.arange(0, len(self.video_list))\n",
    "\n",
    "        #create all possible sampling combinations and put in self.all_idxs\n",
    "        self.all_idxs = []\n",
    "        for num in self.video_nums:\n",
    "            cur_frame_nums = np.random.choice(np.arange(self.video_length - self.frame_length - self.ppg_offset), size=self.num_samps, replace=False)\n",
    "\n",
    "            \n",
    "            for cur_frame_num in cur_frame_nums:\n",
    "                self.all_idxs.append((num,cur_frame_num))\n",
    "\n",
    "\n",
    "    \n",
    "            \n",
    "            \n",
    "    def __len__(self):\n",
    "        return int(len(self.all_idxs))\n",
    "    def __getitem__(self, idx):\n",
    "        video_number, frame_start = self.all_idxs[idx]\n",
    "     \n",
    "        #Get signal\n",
    "        item_sig = self.signal_list[int(video_number)][int(frame_start):int(frame_start+self.frame_length)]\n",
    "\n",
    "        ##Load the 64 necessary frames for attention\n",
    "        folder = self.video_list[video_number]\n",
    "        temp_stor = np.zeros((self.frame_length,128,128,3))\n",
    "        \n",
    "        temp_stor2 = np.zeros((self.video_length,128,128,3))\n",
    "        \n",
    "        ixx = 0\n",
    "        for frm in range(frame_start,frame_start+self.frame_length):\n",
    "            im = imageio.imread(os.path.join(self.datapath, folder,'rgbd_rgb_'+str(frm)+'.png'))\n",
    "\n",
    "            temp_stor[ixx] = im\n",
    "            \n",
    "            ixx+=1\n",
    "            \n",
    "        frmOuts = temp_stor\n",
    "            \n",
    "        for fst in range(self.num_segments):\n",
    "            vid = np.load(os.path.join(self.datapath2, 'residual_'+str(int(fst*self.part_length))+'_'+folder+'.npy'))\n",
    "\n",
    "            temp_stor2[fst*self.part_length:(fst+1)*self.part_length] = vid\n",
    "\n",
    "        frmOuts2 = temp_stor2[frame_start:frame_start+self.frame_length]\n",
    "        \n",
    "        frmOuts = np.concatenate((frmOuts, frmOuts2), axis=-1)\n",
    "\n",
    "        return frmOuts, item_sig\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7z0WpPowS0wg"
   },
   "outputs": [],
   "source": [
    "def extract_video(path, path2, cur_session):\n",
    "    length_seg = 300\n",
    "    vid = np.zeros((900,128,128,3))\n",
    "    \n",
    "    for j in range(900): #number of segments\n",
    "        im = imageio.imread(os.path.join(path, cur_session,'rgbd_rgb_'+str(j)+'.png'))\n",
    "        vid[j] = im\n",
    "    \n",
    "    vid2 = np.zeros((900,128,128,3))\n",
    "    for j in range(3): #number of segments\n",
    "        video_rd = np.load(os.path.join(path2, 'residual_'+str(j*length_seg)+'_'+cur_session+'.npy'))\n",
    "\n",
    "        vid2[j*length_seg:(j+1)*length_seg] = video_rd\n",
    "        \n",
    "    #concatenate\n",
    "    vid = np.concatenate((vid, vid2), axis=-1)\n",
    "\n",
    "    return vid #should be of shape 3x900\n",
    "\n",
    "##Eval model\n",
    "def eval_model(root_dir, root_dir2, session_names, model, model2, in_frames=64, \n",
    "               hr_window_size = 300, stride = 128, video_fps = 30, ppg_offset = 25, fft_resolution=48):\n",
    "    model.eval()#!!!\n",
    "    model2.eval()\n",
    "    video_samples = []\n",
    "    for cur_session in session_names:\n",
    "        video_sample = {\"video_path\" : root_dir, \"video_path2\" : root_dir2, \"cur_session\" : cur_session}\n",
    "        video_samples.append(video_sample)\n",
    "\n",
    "\n",
    "    #Get indices for FFT\n",
    "    #TODO: band limit dependencies. clean this up\n",
    "    l_freq_bpm = 45\n",
    "    u_freq_bpm = 180\n",
    "\n",
    "    for cur_video_sample in tqdm(video_samples):\n",
    "        cur_video_path = cur_video_sample[\"video_path\"]\n",
    "        cur_video_path2 = cur_video_sample[\"video_path2\"]\n",
    "        cur_session = cur_video_sample[\"cur_session\"]\n",
    "\n",
    "        frames = extract_video(path=cur_video_path, path2=cur_video_path2, cur_session=cur_session) # (900, 128, 128, 3)\n",
    "        target = np.load(os.path.join(cur_video_path, cur_session,'rgbd_ppg.npy'))\n",
    "        ##Apply offset to target\n",
    "        target = target[ppg_offset:]\n",
    "\n",
    "\n",
    "\n",
    "        #Normalize target\n",
    "        target = (target-np.mean(target,axis=0,keepdims=True))/np.std(target,axis=0,keepdims=True)\n",
    "\n",
    "        #get the start indices\n",
    "        start_indices = np.arange(0,frames.shape[0]+1-ppg_offset-hr_window_size,stride)\n",
    "        \n",
    "\n",
    "        batched_ip = []\n",
    "        batched_tgt = []\n",
    "\n",
    "        for ix in start_indices:\n",
    "            temp_ip = frames[ix:ix+hr_window_size]\n",
    "\n",
    "            batched_ip.append(temp_ip)\n",
    "\n",
    "\n",
    "            temp_tgt = target[ix:ix+hr_window_size]\n",
    "            \n",
    "            batched_tgt.append(temp_tgt)\n",
    "\n",
    "        batched_ip = torch.Tensor(np.array(batched_ip)).to(device)\n",
    "        batched_tgt = np.array(batched_tgt) \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        # Potential High GPU usage\n",
    "        # A batch_size of approx 14\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            msk = model2(batched_ip[:,0:64])\n",
    "            est_wvfrm = model(batched_ip,msk)\n",
    "            \n",
    "            # (14, 64)\n",
    "            est_wvfrm = est_wvfrm.squeeze().cpu().numpy() #Size: B,fft_size\n",
    "\n",
    "       # Save\n",
    "        cur_video_sample['est_wvfrm'] = est_wvfrm\n",
    "        cur_video_sample['gt_wvs'] = batched_tgt\n",
    "    print('All finished!')\n",
    "\n",
    "    #Estimate using waveforms\n",
    "    mae_list = []\n",
    "    all_hr_est = []\n",
    "    all_hr_gt = []\n",
    "    for index, cur_video_sample in enumerate(video_samples):\n",
    "        cur_video_path = cur_video_sample['video_path']\n",
    "        est_wvfrm = cur_video_sample['est_wvfrm']\n",
    "        \n",
    "        gt_wvs = cur_video_sample['gt_wvs']\n",
    "\n",
    "        #Just need to iterate over batch dimension and\n",
    "\n",
    "        # Get est HR for each window\n",
    "        hr_est_temp = []\n",
    "        hr_gt_temp = []\n",
    "        for ixx in range(est_wvfrm.shape[0]):\n",
    "            est_hr = prpsd2(est_wvfrm[ixx], 30, 45, 150, BUTTER_ORDER=6, DETREND=False)\n",
    "            gt_hr = prpsd2(gt_wvs[ixx], 30, 45, 150, BUTTER_ORDER=6, DETREND=False)\n",
    "            hr_est_temp.append(est_hr)\n",
    "            hr_gt_temp.append(gt_hr)\n",
    "\n",
    "        hr_est_windowed = np.array([hr_est_temp])\n",
    "        hr_gt_windowed = np.array(hr_gt_temp)\n",
    "        all_hr_est.append(hr_est_temp)\n",
    "        all_hr_gt.append(hr_gt_temp)\n",
    "\n",
    "        # Errors\n",
    "        RMSE, MAE, MAX, PCC = getErrors(hr_est_windowed, hr_gt_windowed)\n",
    "\n",
    "        mae_list.append(MAE)\n",
    "    print('Mean MAE:', np.mean(np.array(mae_list)))\n",
    "    return np.array(mae_list), (all_hr_est, all_hr_gt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1H2MspBw_Ck"
   },
   "outputs": [],
   "source": [
    "\n",
    "def prpsd2(BVP, FS, LL_PR, UL_PR, BUTTER_ORDER=6, DETREND=False, PlotTF=False, FResBPM = 0.1):\n",
    "    '''\n",
    "    Estimates pulse rate from the power spectral density a BVP signal\n",
    "    \n",
    "    Inputs\n",
    "        BVP              : A BVP timeseries. (1d numpy array)\n",
    "        fs               : The sample rate of the BVP time series (Hz/fps). (int)\n",
    "        lower_cutoff_bpm : The lower limit for pulse rate (bpm). (int)\n",
    "        upper_cutoff_bpm : The upper limit for pulse rate (bpm). (int)\n",
    "        butter_order     : Order of the Butterworth Filter. (int)\n",
    "        detrend          : Detrend the input signal. (bool)\n",
    "        FResBPM          : Resolution (bpm) of bins in power spectrum used to determine pulse rate and SNR. (float)\n",
    "    \n",
    "    Outputs\n",
    "        pulse_rate       : The estimated pulse rate in BPM. (float)\n",
    "    \n",
    "    Daniel McDuff, Ethan Blackford, January 2019\n",
    "    Copyright (c)\n",
    "    Licensed under the MIT License and the RAIL AI License.\n",
    "    '''\n",
    "    from scipy.signal import butter\n",
    "    from scipy import signal\n",
    "    import numpy as np\n",
    "\n",
    "    N = (60*FS)/FResBPM\n",
    "\n",
    "\n",
    "    [b, a] = signal.butter(BUTTER_ORDER, [LL_PR/60, UL_PR/60], btype='bandpass', fs = FS)\n",
    "    \n",
    "    BVP = signal.filtfilt(b, a, np.double(BVP))\n",
    "    \n",
    "\n",
    "    F, Pxx = signal.periodogram(x=BVP,  nfft=N, fs=FS, detrend=False);  \n",
    "\n",
    "    FMask = (F >= (LL_PR/60)) & (F <= (UL_PR/60))\n",
    "    \n",
    "    # Calculate predicted pulse rate:\n",
    "    FRange = F * FMask\n",
    "    PRange = Pxx * FMask\n",
    "    MaxInd = np.argmax(PRange)\n",
    "    pulse_rate_freq = FRange[MaxInd]\n",
    "    pulse_rate = pulse_rate_freq*60\n",
    "\n",
    "    # Optionally Plot the PSD and peak frequency\n",
    "    if PlotTF:\n",
    "        # Plot PSD (in dB) and peak frequency\n",
    "        plt.figure()\n",
    "        plt.plot(F, 10 * np.log10(Pxx))\n",
    "        plt.plot(pulse_rate_freq, 10 * np.log10(PRange[MaxInd]),'ro')\n",
    "        plt.xlabel('Frequency (Hz)')\n",
    "        plt.ylabel('Power (dB)')\n",
    "        plt.xlim([0, 4.5])\n",
    "        plt.title('Power Spectrum and Peak Frequency')\n",
    "            \n",
    "    return pulse_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 213856,
     "status": "ok",
     "timestamp": 1672890126548,
     "user": {
      "displayName": "Pradyumna Chari",
      "userId": "06555723454452599039"
     },
     "user_tz": 480
    },
    "id": "lsfi3Q5OgPgp",
    "outputId": "45760260-c7db-4676-9f60-ef6fe8da2314",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "destination_folder = \"VideoDataset/rgb_files/\"\n",
    "destination_folder2 = \"ResidualData/NpyRes/\"\n",
    "\n",
    "\n",
    "with open(\"assets/demo_fold.pkl\", \"rb\") as fpf:\n",
    "        out = pickle.load(fpf)\n",
    "\n",
    "train = out[0][\"train\"]\n",
    "val = out[0][\"val\"]\n",
    "test = out[0][\"test\"]\n",
    "\n",
    "#Dataset\n",
    "dataset = RppgData2(datapath=destination_folder, datapath2=destination_folder2, datapaths=train, recording_str=\"rgbd_rgb\")\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1672890127476,
     "user": {
      "displayName": "Pradyumna Chari",
      "userId": "06555723454452599039"
     },
     "user_tz": 480
    },
    "id": "UnUIKN4BlN60",
    "outputId": "be258801-3b6a-4d00-b761-e64b7687eee5"
   },
   "outputs": [],
   "source": [
    "#Set Checkpoint Directory\n",
    "colab_filename = 'Example_Train_Run'\n",
    "ckpt_parent_path = 'checkpoints_mask'\n",
    "assert os.path.exists(ckpt_parent_path), \"Check folder to save checkpoint\"\n",
    "ckpt_path = os.path.join(ckpt_parent_path, colab_filename)\n",
    "os.makedirs(ckpt_path, exist_ok=True)\n",
    "print(f\"Checkpoints will be saved in {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_BxlO_PRUP23"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class PlethRegressor(nn.Module):\n",
    "    def __init__(self, inp_len, out_len, latent=512):\n",
    "        super(PlethRegressor, self).__init__()\n",
    "        self.inp_len = inp_len\n",
    "        self.latent = latent\n",
    "        self.out_len = out_len\n",
    "        self.Enc1 = nn.Sequential(\n",
    "            nn.Conv1d(6, 64, kernel_size=9, stride=1,padding=4),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.Enc2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=9, stride=1,padding=4),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.Enc3 = nn.Sequential(\n",
    "            nn.Conv1d(128, 128, kernel_size=9, stride=1,padding=4),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.Enc4 = nn.Sequential(\n",
    "            nn.Conv1d(128, 128, kernel_size=9, stride=1,padding=4),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.Dec1 = nn.Sequential(\n",
    "            nn.Conv1d(128, 128, kernel_size=9, stride=1,padding=4),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.Dec2 = nn.Sequential(\n",
    "            nn.Conv1d(128, 128, kernel_size=9, stride=1,padding=4),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.Dec3 = nn.Sequential(\n",
    "            nn.Conv1d(128, 64, kernel_size=9, stride=1,padding=4),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.Dec4 = nn.Sequential(\n",
    "            nn.Conv1d(64, 16, kernel_size=9, stride=1,padding=4),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.FinalLayer   = nn.Conv1d(16, 1, kernel_size=1, stride=1)\n",
    "    def forward(self, inp, wts):\n",
    "        wts = wts.reshape(wts.shape[0],1,wts.shape[1],wts.shape[2],1)\n",
    "        inp = torch.sum(inp*wts,(2,3))/torch.sum(wts,(2,3))\n",
    "        inp = inp.permute(0,2,1)\n",
    "        \n",
    "        ##Mean and std normalization\n",
    "        inp = (inp - torch.mean(inp, 2, True))/torch.std(inp, dim=2, keepdim=True)\n",
    "\n",
    "        inp = self.Enc1(inp)\n",
    "        inp = self.Enc2(inp)\n",
    "        inp = self.Enc3(inp)\n",
    "        inp = self.Enc4(inp)\n",
    "        inp = self.Dec1(inp)\n",
    "        inp = self.Dec2(inp)\n",
    "        inp = self.Dec3(inp)\n",
    "        inp = self.Dec4(inp)\n",
    "        output_signal = self.FinalLayer(inp)\n",
    "        return torch.squeeze(output_signal, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDyYvb2LMzhA"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.utils import _triple\n",
    "import torch as tr\n",
    "import pdb\n",
    "\n",
    "class SNRLoss_dB_Signals(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SNRLoss_dB_Signals, self).__init__()\n",
    "  def forward(self, outputs: tr.Tensor, targets: tr.Tensor, Fs=30):\n",
    "    device = outputs.device\n",
    "    if not outputs.is_cuda:\n",
    "      torch.backends.mkl.is_available()\n",
    "    N = 600\n",
    "    N_samp = outputs.shape[-1]\n",
    "    pulse_band = tr.tensor([45/60., 180/60.], dtype=tr.float32).to(device)\n",
    "    wind_sz = int(1*N/64)\n",
    "\n",
    "    f = tr.linspace(0, Fs/2, int(N/2)+1, dtype=tr.float32).to(device)\n",
    "    min_idx = tr.argmin(tr.abs(f - pulse_band[0]))\n",
    "    max_idx = tr.argmin(tr.abs(f - pulse_band[1]))\n",
    "\n",
    "    outputs = outputs.view(-1, N_samp)\n",
    "    targets = targets.view(-1, N_samp)\n",
    "\n",
    "    #Generate GT heart indices from GT signals\n",
    "    Y = torch.fft.rfft(targets, n=N, dim=1, norm='forward')\n",
    "    Y2 = tr.abs(Y) ** 2\n",
    "    HRixs = tr.argmax(Y2[:,min_idx:max_idx],axis=1)+min_idx\n",
    "\n",
    "    #print(outputs.shape)\n",
    "    X = torch.fft.rfft(outputs, n=N, dim=1, norm='forward')\n",
    "\n",
    "    P1 = tr.abs(X) ** 2\n",
    "\n",
    "    # calc SNR for each batch\n",
    "    losses = tr.empty((X.shape[0],), dtype=tr.float32)#.to(device)\n",
    "    for count, ref_idx in enumerate(HRixs):\n",
    "      pulse_freq_amp = tr.sum(P1[count, ref_idx-wind_sz:ref_idx+wind_sz])+tr.sum(P1[count, 2*ref_idx-wind_sz:2*ref_idx+wind_sz])\n",
    "      other_avrg = (tr.sum(P1[count, min_idx:ref_idx-wind_sz])+tr.sum(P1[count, ref_idx+wind_sz:2*ref_idx-wind_sz]) + tr.sum(P1[count, 2*ref_idx+wind_sz:max_idx]))\n",
    "      losses[count] = -10*tr.log10(pulse_freq_amp/(other_avrg+1e-7))\n",
    "    losses.to(device)\n",
    "    return tr.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(img):\n",
    "     bs_img, h_img, w_img = img.size()\n",
    "     tv_h = torch.pow(img[:,1:,:]-img[:,:-1,:], 2).sum()\n",
    "     tv_w = torch.pow(img[:,:,1:]-img[:,:,:-1], 2).sum()\n",
    "     return (tv_h+tv_w)/(bs_img*h_img*w_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 795162,
     "status": "error",
     "timestamp": 1672890922635,
     "user": {
      "displayName": "Pradyumna Chari",
      "userId": "06555723454452599039"
     },
     "user_tz": 480
    },
    "id": "8_ri0LHHhdW0",
    "outputId": "cd82f79d-1f57-4232-c61a-1c46e21ff560",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "#CONTEXT PATH\n",
    "PATH = os.path.join(os.getcwd(), f\"{ckpt_path}/latest_context.pth\")\n",
    "model = PlethRegressor(inp_len=300, out_len=300).to(device)\n",
    "model2 = CNN3D(frames=64, sidelen = 128, channels=6).to(device)\n",
    "\n",
    "loss_fn  = Neg_Pearson2()\n",
    "loss_fn2  = SNRLoss_dB_Signals()\n",
    "\n",
    "lam = 3\n",
    "lam2 = 5\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "\n",
    "# Train configurations\n",
    "epochs = 2\n",
    "checkpoint_period = 1\n",
    "epoch_start = 1\n",
    "mae_best_loss = 1000\n",
    "\n",
    "for epoch in range(epoch_start, epochs+1):\n",
    "    # Training Phase\n",
    "    loss_train = 0\n",
    "    no_batches = 0\n",
    "    for batch, (imgs, signal) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        model.train()\n",
    "        model2.train()\n",
    "\n",
    "        imgs = imgs.float().to(device)\n",
    "        signal = signal.float().to(device)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Get attention mask\n",
    "        msk = model2(imgs[:,0:64])\n",
    "\n",
    "        # Predict the PPG signal and find ther loss\n",
    "        pred_signal = model(imgs,msk)\n",
    "        \n",
    "        loss1 = loss_fn(pred_signal, signal)\n",
    "        loss2 = loss_fn2(pred_signal, signal)\n",
    "        loss3 = total_variation_loss(msk)\n",
    "        \n",
    "        loss = loss2+lam*loss1+lam2*loss3\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer2.step()\n",
    "        \n",
    "        # Accumulate the total loss\n",
    "        loss_train += loss.item()\n",
    "        no_batches+=1\n",
    "\n",
    "    # Save the model every few epochs\n",
    "    if(epoch % checkpoint_period == 0):\n",
    "        torch.save(model.state_dict(), os.path.join(os.getcwd(), f\"{ckpt_path}/PhysNet_state_dict_{epoch}_epochs.pth\"))\n",
    "        torch.save(model2.state_dict(), os.path.join(os.getcwd(), f\"{ckpt_path}/PhysNetAtt_state_dict_{epoch}_epochs.pth\"))\n",
    "        #See if best checkpoint\n",
    "        mae_loss_list, hrs = eval_model(root_dir=destination_folder, root_dir2=destination_folder2, session_names=val, model=model, model2=model2)\n",
    "        current_loss = np.mean(mae_loss_list) \n",
    "        if(current_loss < mae_best_loss):\n",
    "            mae_best_loss = current_loss\n",
    "            torch.save(model.state_dict(), os.path.join(os.getcwd(), f\"{ckpt_path}/PhysNet_state_dict_best.pth\"))\n",
    "            torch.save(model2.state_dict(), os.path.join(os.getcwd(), f\"{ckpt_path}/PhysNetAtt_state_dict_best.pth\"))\n",
    "            print(\"Best checkpoint saved!\")\n",
    "        print(\"Saved Checkpoint!\")\n",
    "\n",
    "    print(f\"Epoch: {epoch} ; Loss: {loss_train/no_batches:>7f}\")\n",
    "    \n",
    "    #plot an example mask\n",
    "    ixx = 0\n",
    "    og_msk = msk[ixx].cpu().detach().numpy()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(og_msk)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1C9C7CGXx-5-Y-GLQaHor0AyTR5ebB9T_",
     "timestamp": 1639989285816
    }
   ]
  },
  "kernelspec": {
   "display_name": "hashppg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b310efec2753918fc2ffe1b7b62e4629d4af86145c6b3c505a1f8291347894dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
